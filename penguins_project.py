# -*- coding: utf-8 -*-
"""Penguins_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qb9xMfP7d_FLYXyLwPZoi4Fwxf5LmTQl

# PIC 16A Final Project: Penguins

> WRITTEN BY: Katherine Wang, Joshua Yamane, Kira Curry

# 1. Group contributions statement

Katherine wrote the data acquisition code, data preparation code, and the affiliated docstrings and comments for everything under '2. Data import and cleaning'.

Katherine, Kira, Josh wrote the explorary analysis codes. Josh wrote the descriptions and comments for everything under '3. Exploratory analysis'.

Katherine led Figure 1, the multinomial logistic regression model, and the explanations of everything under Figure 1. Kira led Figure 2,  the random forests model, and the explanations of everything under Figure 2. Josh led Figure 3, the nearest neighbor classifier model, and the explanations of everything under Figure 3. Kira wrote the concluding discussion section.

We all checked each otherâ€™s work, added additional comments/descriptions, and made revisions to code and writing.

# 2. Data import and cleaning
"""

#importing the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import graphviz
from matplotlib import pyplot as plt
from sklearn import tree, preprocessing
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
#from sklearn.tree import DecisionTreeClassifier, export_graphviz

#Code to Read the Data and Mapping the Data Frame
url = "https://philchodrow.github.io/PIC16A/datasets/palmer_penguins.csv"
penguins = pd.read_csv(url)
penguins

print(penguins['Sex'].unique())
penguins['Sex'] = penguins['Sex'].replace(['nan', '.', '', 'NaN'], pd.NA)
penguins = penguins[penguins['Sex'].notna()]
print(penguins['Sex'].unique())

"""This section of code examines the 'Sex' column and identifies the unique values within the column.
The first line prints all the unique values within the code and the second and third lines of code
replace the 'nan' and '.' with actual "NaN" values. From here, we subset the 'Sex' column to be equal to everything except the NaN values which we have
just converted. The last line of code confirms that all that is left is MALE and FEMALE.

We are splitting the data into train and test before cleaning/prepping it. Data cleaning, data exploration, or other operations occur AFTER train-test split.
"""

#Separating the data into training and testing
train, test = train_test_split(penguins, test_size = 0.2, random_state = 42)
train.shape, test.shape

"""After we removed the NaN and other irrelevant values from the penguins dataset, we split the data into our training and test data here using train_test_split."""

# Function to apply label encoding to a column in both train and test datasets
le = preprocessing.LabelEncoder()

def apply_label_encoding(train, test, columns):
    '''
    Function applies label encoding to specified columns in the train and test datasets.

    Parameters:
    - train: training dataset
    - test: testing dataset
    - columns: the list of column names to apply label encoding to

    Returns:
    - train: modified training dataset with label encoded columns
    - test: modified testing dataset with label encoded columns

    '''
    for col in columns:
        train[col] = le.fit_transform(train[col])
        test[col] = le.transform(test[col])  # use transform to avoid refitting the encoder
    return train, test

# Function to map categorical features (like 'Island') to numeric values
def apply_category_mapping(train, test, column, mapping_dict):
    '''
    Function maps categorical values in a specified column to numeric values based on the given mapping dictionary.

    Parameters:
    - train: training dataset
    - test: testing dataset
    - column: string, name of the column to apply the mapping to
    - mapping_dict: a dict that maps categorical values to numeric values

    Returns:
    - train: the modified training dataset with mapped categorical column
    - test: the modified testing dataset with maped categorcial colunm

    '''
    train[column] = train[column].map(mapping_dict)
    test[column] = test[column].map(mapping_dict)
    return train, test

# Function to prepare the dataset by dropping unwanted columns
def prep_penguin(data):
    '''
    Function prepares the penguin dataset by dropping unnecessary columns and splitting it into features (X) and target (y).

    Parameters:
    - data (DataFrame): The dataset to be preprocessed.

    Returns:
    - X: feature matrix after dropping unnecessary cols
    - y: target vector containing 'Species' col

    '''
    df = data.copy()
    columns_to_drop = ['studyName', 'Comments', 'Clutch Completion', 'Individual ID',
                       'Date Egg', 'Stage', 'Sample Number']
    df.drop(columns=columns_to_drop, axis=1, inplace=True)
    X = df.drop('Species', axis=1)
    y = df['Species']
    return X, y

# Apply label encoding to 'Sex' and 'Region'
train, test = apply_label_encoding(train, test, ['Sex', 'Region'])

# Apply Island mapping
island_map = {'Biscoe': 0, 'Torgersen': 1, 'Dream': 2}
train, test = apply_category_mapping(train, test, 'Island', island_map)

"""In this section of code, we continue to preprocess the data by using LabelEcoder to encode our qualitative variables like island and sex. We also create the function called prep_penguin() to drop the columns we deem will be irrelevant from the start. This includes columns such as the studyName, Individual ID, Sample Number, etc."""

X_train, y_train = prep_penguin(train) #Creating the training and test data variables
X_test, y_test = prep_penguin(test)

"""# 3. Exploratory Analysis"""

X_train.head() #This provides a quick overview and check to ensure for our X_train is prepared to be analyzed

y_train.head() #Like the previous line of code, this provides a quick check to make sure our training response variable is working properly

#Computing summary statitsics that provide the count, mean standard deviations, minimums, maximums, and percentiles of each of the columns of penguin data.
X_train.describe()

train.drop(columns=['Sample Number'], axis=1, inplace=True) #@Katherine, the Sample Number was popping up as a column but it should've been dropped. I went ahead and manually dropped it but if you could look at the code in the function to see why it's not being dropped there that'd be great
summary = train.groupby('Species').mean(numeric_only=True)
summary

"""This table summarizes the average statistics of the variables of each species we would like to look at. As a reminder, we have already removed some columns from the data frame that we deemed unecessary in the determination of each species. Here, we are left with a preliminary look at to what the average measurements are to see if we can parse out any initial clusterings among the species. Adelie penguins seem to have a much smaller average culmen length than the Chinstrap and Gentoo making this a variable of interest. For Culmen Depth, Adelie and Chinstrap have relatively similar average culmen depths however the Gentoo penguin has a noticably lower average culmen depth by about 4mm. Flipper length could also be a point of interst as each species seems to be clustered around its own point. Body mass will also likely be a variable of interst as the Gentoo is on average much larger (around 1300g) than the Adelie and Chinstrap. However, this is just preliminary EDA and more research into selecting what variables will need to be done."""

# Select numerical columns
numerical_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)',"Delta 15 N (o/oo)",'Delta 13 C (o/oo)' ]

# Create a scatterplot matrix
pairplot = sns.pairplot(data=train, vars=numerical_cols, hue="Species", diag_kind="kde")

# Show the plot
plt.suptitle("Scatterplot Matrix with Correlations", y=1.02)  # Add a title
plt.show()

"""This scatterplot matrix places all the variables of interest on a matrix. We map each variable against each other and by clustering by species, we can look for patterns in the data with the most distinct clustering. As seen in the first row of Culmen Length (mm), the first three scatterplots in this row all show fairly strong signs of clustering. The interaction in the first row between Delta 15 N (o/oo) and Delta 13 C (o/oo) show signs of clustering but not as strong as the relationship between Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), and Body Mass (g). Looking over the other plots, there are no other real clear signs of clustering the way there is in the first row."""

# We NEED to keep a consitent color theme in this. Adelie is a diff color in the island dist compared to the sex and region
# Count of species by Island
sns.countplot(data=train, x='Island', hue='Species')
plt.title("Species Distribution by Island")
plt.show()

# Count of penguins by Sex
sns.countplot(data=train, x='Sex', hue='Species')
plt.title("Species Distribution by Sex")
plt.show()

# Count of penguins by Region
sns.countplot(data=train, x='Region', hue='Species')
plt.title("Species Distribution by Region")
plt.show()

"""This code creates a histogram of the qualitative variables. The y-axis of all the graphs represents the count of each species for each respective qualitative variable. The key for each island is as follows: Biscoe: 0, Torgersen: 1, Dream: 2. From these graphs, we can conclude that region will not be worth researching more into. Because all species are found within the Anvers region, this would provide no significance as to determining what penguin species we might find. Examining species distribution by island will be useful since all of the species are only found on one island besides the Adelie which is found on all three. Sex should also be looked at since this each species seems to have a similar number of males to females."""

# Boxplot for Body Mass by Island
#We Could potentially automate this? Or make a variable and then create a for loop that looks iterates through all the variables so we aren't copy/pasting
sns.boxplot(data=train, x='Island', y='Culmen Length (mm)', hue='Species')
plt.title("Culmen Length by Island and Species")
plt.show()

"""Looking at this graph we can see that there are clear clusters in the average Culmen Length (mm) between species among the different islands. For instance, without any modeling, a loose prelimary classification can be made that a penguin with a culmen length of ~46mm found on the island of Biscoe will most likely be a Gentoo penguin. Again, this without any modeling but initial intuition can be formed surrounding our variables and their importance. This proves that Culmen Length (mm) and Island will be useful in selecting which features we want."""

plt.figure(figsize=(5, 5))
sns.boxplot(
    x="Island", y="Culmen Depth (mm)",data=train,
    hue = "Species",legend=True
)
plt.show()

plt.figure(figsize=(7, 5))
sns.boxplot(
    #x="Species", y="Delta 15 N (o/oo)",data=train,
    y="Delta 15 N (o/oo)", x="Island",data=train,
    legend=True, hue = "Species"
)
plt.show()

plt.figure(figsize=(7, 5))
sns.boxplot(
    x="Island", y ="Delta 13 C (o/oo)",data=train, hue = "Species"
)
plt.show()

"""While there is some clustering among the groups, it is not nearly as prominent as it was in the Culmen Length (mm) and Culmen Depth (mm). For these reasons, our group has decided to only focus on the island and sex for our qualitative variables and Culmen Length (mm), Culmen Depth (mm), and Flipper Length (mm) for our quantitative variables. These have shown the most most distinct patterns of species clustering and as such will act as good indicators to base our models on. Because of the evidence shown in the initial average summary table, scatterplot matrix, qualitative histograms, and boxplots, we want to base our models on the features that will contribute the most to correctly identifying each species and these are the variables we will use in our models.

# 4. Feature selection

Multinominal logistic regression model:
*   **qualitative variable**: Sex
*   **quantitative 1 variable**: Culmen depth (mm)
*   **quantitative 2 variable**: Delta 15 N (o/oo)

Random forests model:
*   **qualitative variable**: Region
*   **quantitative 1 variable**: Flipper Length (g)
*   **quantitative 2 variable**: Delta 13 C (o/oo)

Nearest neighbor classifier model:
*   **qualitative variable**: Sex
*   **quantitative 1 variable**: Culmen depth (mm)
*   **quantitative 2 variable**: Delta 15 N (o/oo)

# 5. Modeling

## Figure 1: Multinominal Logistic Regression
"""

# Quick review of what X_train contains
X_train.head()

# Quick review of what y_train contains
y_train.head()

# Creating new X and y variables for LR model from X_train and y_train
X = X_train[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island']]
y = y_train

# Confirming X is what I'm looking for (two quant_var, one qual_var)
X.head()

# Confirming y is what I want (Species)
y.head()

# Importing  Logistic Regression model and cross-validation scoring func from scikit-learn
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# Initializing and fitting Logistic Regression model for multi-class classification
LR = LogisticRegression(multi_class='multinomial')
LR.fit(X,y)

# Reviewing y variable 'Species', dtype object, string not int/float
y

# Mapping y ("Species") column values to numerical values (y_numerical) for plotting LR
    # using map() and not encoding because y dtype is object, not dataframe
    # LR modeling requires int/float type variables only

unique_species = y.unique()
print(unique_species)
species_mapping = {species: idx for idx, species in enumerate(unique_species)}
y_numerical = y.map(species_mapping)
print(y_numerical)

# First looking only at quantitative variables
    # Selecting "Culmen Length (mm)" and "Culmen Depth (mm)" columns from X
    # Note: thridd variable in X ("Island") is qualitative and will be added later
x0=X["Culmen Length (mm)"]
x1=X["Culmen Depth (mm)"]

# Creating a figure and axis for scatter plot
fig,ax=plt.subplots(1)
ax.scatter(x0,x1,c=y_numerical)

# Set labels to clarify what x and y axes represent
ax.set(xlabel="Culmen Length (mm)",ylabel="Culmen Depth (mm)")

# Adding qualitative variable "Island" to predict "Species'
# Function to plot decision boundaires
# Note: I closely referenced the notebook from Week 8 Lecture 23, 'decisionregion-1' for this func

def plot_regions(c, X, y, quant1="Culmen Length (mm)", quant2="Culmen Depth (mm)", qual_var="Island"):
    '''
    Function plots the decisoin boundaries for classifier `c` over a 2D feature space defined by two
    quantitative variables, and  also accounts for a qualitative variable (`qual_var`).
    The decision boundaries are plotted for each possible value of the qualitative variable.

    Args:
      - c: The classifier (a trained model like logistic regression LR)
      - X (df): containing the predictor data (quantitative and qualitative variables)
      - y (object): The target variable, used for coloring the points in the scatter plot
      - quant1 (str): first quantitative feature
      - quant2 (str): second quantitative feature
      - qual_var (str): The qualitative feature

    Returns:
      None.

    '''

    # Extract values of quant variabels: Culmen Length and Culmen Depth
    x0 = X[quant1].values
    x1 = X[quant2].values

    # Defining a range for each quant variable
    grid_x = np.linspace(x0.min(), x0.max(), 501)
    grid_y = np.linspace(x1.min(), x1.max(), 501)
    # meshgrid for the two ranges to evaluate 2D grid predictions
    xx, yy = np.meshgrid(grid_x, grid_y)

    # Looping through unique values of qual_var (values 0, 1, 2)
    for current_island in range(3):  # Island values: 0, 1, 2
        print(f"Decision boundaries for island {current_island}")

        # Flattening the meshgrid arrays (xx and yy)
        # to create a list of 2D points
        XX = xx.ravel()
        YY = yy.ravel()

        # Combining the fixed value of qual_var to grid
        grid_data = np.c_[XX, YY, np.full_like(XX, current_island)]  # Adding the fixed island value

        # Predict class labels for each point in grid using classifier c
        p = c.predict(grid_data)

        # Convert any categorical predictions to numerical
        if not np.issubdtype(p.dtype, np.number):
            p = pd.factorize(p)[0]  # Convert to numerical labels

        # Reshape predictions array to match shape of meshgrid, prepare for plotting
        p = p.reshape(xx.shape)

        fig, ax = plt.subplots(figsize=(8, 6))

        # Plotting the decision regions
        ax.contourf(xx, yy, p, alpha=0.3, cmap="jet")

        # Plot the original data points on top of the decision regions
        scatter = ax.scatter(x0, x1, c=y_numerical, edgecolors='k', s=50)
        ax.set_xlabel(quant1)
        ax.set_ylabel(quant2)
        ax.set_title(f'Decision Boundaries for {quant1} and {quant2} (Island = {current_island})')
        plt.colorbar(scatter)
        plt.show()

"""Recall island mapping:

*   0 : Biscoe
*   1 : Torgersen
*   2 : Dream
"""

qual_var = "Island"
LR = LogisticRegression(multi_class='multinomial', max_iter=200).fit(X, y)

# Call the plot_regions function to plot decision boundaries
plot_regions(LR, X, y_numerical, qual_var=qual_var)

# Training model five times, then compute mean of accuracy scores
cross_val_score(LR, X, y, cv = 5).mean()

"""**Multinominal LR accuracy score = 0.9888.** This is a very good score. The model correctly predicted the target variable for 98.88% of the test samples during cross-validation.

From the three plots, we can see the decision boundaries for Island 1 (Torgersen) is most relevant and accurate.
"""

# Assessing accuracy score with test data using confusion_matrix and classification_report


Xtest = X_test[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island']]
ytest = y_test

# Quick review to ensure Xtest and ytest contain desired values (length 67, not 266)
print(len(Xtest))
print(len(ytest))

print(confusion_matrix(ytest, LR.predict(Xtest)))

"""Recall island mapping:

*   0 : Adelie Penguin (Pygoscelis adeliae)
*   1 : Chinstrap penguin (Pygoscelis antarctica)
*   2 : Gentoo penguin (Pygoscelis papua)

**Discussion/Interpretation of Multinominal LR Performance: Confusion Matrix**

The confusion matrix reveals the following:

Adelie penguins (mapped as 0):

*   The model correctly predicted 31 instances as Adelie penguins (True Positive = 31).
*   There are 0 instances where the model incorrectly predicted Adelie when it was actually another species (False Positive = 0).
*   There are 0 instances where the model predicted a species other than Adelie when it was actually Adelie (False Negative = 1).
*   There are 36 instances where Chinstrap and Gentoo penguins correctly predicted as NOT Adelie (True Negative = 36).


Chinstrap penguins (mapped as 1):

*   The model correctly predicted 13 instances as Chinstrap penguins (True Positive = 13).
*   There are 0 instances where the model incorrectly predicted Chinstrap when it was actually another species (False Positive = 0).
*   There are 0 instances where the model predicted a species other than Chinstrap when it was actually Chinstrap (False Negative = 0).
*   There are 54 instances where Adelie and Gentoo penguins correctly predicted as NOT Chinstrap (True Negative = 54).

Gentoo penguins (mapped as 1):

*   The model correctly predicted 21 instances as Gentoo penguins (True Positive = 21).
*   There are 0 instances where the model incorrectly predicted Gentoo when it was actually another species (False Positive = 0).
*   There is 0 instances where the model predicted a species other than Gentoo when it was actually Gentoo (False Negative = 0).
*   There are 45 instances where Adelie and Chinstrap penguins correctly predicted as NOT Gentoo (True Negative = 45).
"""

print(classification_report(ytest, LR.predict(Xtest)))

"""**Discussion/Interpretation of Multinominal LR Performance: Classification Report**

High accuracy of 99% suggests the model is highly reliable. There is 100% accuracy performance for Gentoo penguins and very good performance for Adelie penguins (f1 score = 0.99). For Chinstrap pemnguins, there is also considerably good performance with an f1 score of 0.96, with perfect 1.00 recall score but relatively lower precision score compared to other results. The macro avg and weighted avg together show strong overal performance of the multinominal LR.
"""

# Calculating model coefficients for interpretation of results
coefficients = pd.DataFrame(LR.coef_, columns=X.columns, index=LR.classes_)
print(coefficients)

"""**Discussion/Interpretation of Multinominal LR Performance: Coefficients Model**

Adelie penguins (baseline):
*   Culmen Length: The negative coefficient (-1.024673) suggests that as the culmen length increases, the likelihood of an observation being classified as an Adelie Penguin decreases. This means that shorter culmen kengths are more likely to be an Adelie penguin.
*   Culmen Depth: The positive coefficient (1.550944) suggests that as the Culmen Depth increases, the likelihood of an observation being classified as an Adelie penguin increases. Deeper culmen depths are more likely to be associated with an Adelie penguin.
*   Island: The negative coefficient (-0.454923) indicates that a lower Island value is associated with Adelie penguins (considering mapping where Adelie = 0), which suggests that the model has learned that Adelie penguins are more likely to be found on certain islands.

Chinstrap penguins:
*   Culmen Length: The positive coefficient (0.450260) indicates that as the culmen length increases, the likelihood of an observation being classified as a Chinstrap increases, so longer culmen lengths are more likely to correspond to a Chinstrap penguin.
*   Culmen Depth: The negative coefficient (-0.270707) suggests that as the culmen depth increases, the likelihood of an observation being classified as a Chinstrap Penguin decreases. More shallow culmen depths are more likely to be associated with Chinstraps.
*   Island: The large positive coefficient (1.924970) suggests that Chinstraps are more likely to be found on a particular island. This indicates that Chinstrap penguins are strongly associated 'Island' predictor among the penguins.

Gentoo penguins:
*   Culmen Length: The positive coefficient (0.574413) suggests that as the culmen length increases, the likelihood of an observation being classified as a Gentoo penguin increases. Longer culmen lengths are more likely to be associated with Gentoos.
*   Culmen Depth: the negative coefficient (-1.280238) indicates that as the culmen depth increases, the likelihood of an observation being classified as a gentoo decreases. More shallow culmen depths are more likely to be associated with a Gentoo penguin.
*   Island: The negative coefficient (-1.470047) suggests that Gentoo oenguins are less likely to be found on specific islands, or certain island categories are more likely associated with other species like Adelie or Chinstrap penguins.

**Dicussion: Overall Predictability of X variables on Species**

Culmen Length >> It is a feature that can distinguish between Adelie and Chinstrap/Gentoo penguins, with Adelie penguins having relatively shorter culmens.

*   Adelie penguins are more likely to have shorter culmen lengths(negative coef)
*   Chinstrap penguins are more likely to have longer culmen lengths (positive coef)
*   Gentoo penguins also tend to have longer culmen lengths (positive coef)

Culmen Depth >> It is a distinguishing feature, with Adelie penguins having deeper bills compared to the other two species.
*   Adelie penguins have a strong positive association with deeper cylmen depths (positive coef).
*   Chinstrap penguins tend to have more shallow culmen depths (negative coef)
*   Gentoo penguins are also more likely to have shallow culmen depths (negative coefficient)

Island >> It plays a significant role in distinguishing between the Chinstrap and Gentoo penguins, while it is a bit less useful for identifying Adelies.
*   Adelie Penguins have a negative association with a certain Island category (meaning they're less likely found on certain islands)
*   Chinstrap Penguins highly associated with a particular Island (positive coef), suggesting that they are more likely to be found on a certain island
*   Gentoo Penguins are negatively associated with Island (meaning they are less likely to be found on certain island)

## Figure 2: Random Forests
"""

#This section is ised to design and fit the random forest.
X = X_train[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',
                     'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',
                     'Island', 'Sex', 'Region']]
y = y_train
feature_names=["Culmen Length (Standardized)", "Culmen Depth (Standardized)"]
random_forest= RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train, y_train)

#The below section focuses on predicting the random forest.
prediction=random_forest.predict(X_test)
accuracy=accuracy_score(y_test,prediction)
print("The accuracy of the random forest is:", accuracy)
i=0
tree=random_forest.estimators_[i]

#Confusion Matrix
print("The confusion matrix of the random forest is: ")
print(confusion_matrix(y_test, prediction))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, prediction))

#Cross Validation Score
print("The cross validation of the random forest is: ",cross_val_score(random_forest, X_train, y_train, cv = 5).mean())

#This portion is being used to create the random forest graphic
figure=graphviz.Source(
export_graphviz(tree,
feature_names=X_train.columns,
max_depth=8, filled=True
)
)
display(figure)

"""**Discussion/Interpretation of Random Forests: Overall Accuracy:**

The purpose of creating the random forests model was to utilize machiene learning in order to create three trees that would act as a compairssion of the three different islands that were reflected in the data.

1.   The figure contained a variety of gini scores, with the majority being low gini scores that were in close proximity to zero. These represented features such as Body Mass(g), Delta 15 N and Delta 13 C.Likewise, some features had scores that could be considered higher at scores around 0.44, in particular Culmen Length(mm). This can illustrate that  body mass was a successful node in contrast to Culmen Length (mm) where the data was more evenly split. However it is important to note that Culmen Length(mm) was the dominant class as it has 108 samples whereas body mass(g) only has 59 samples.




The accuracy of the random forest is 0.9850746268656716, which can also be reflected as 98.5% and this could be considered a high accuracy model.

**Discussion/Interpretation of Random Forests: Confusion Matrix**

The confusion matrix of the random forest, can be broken down into three row segments.


1.   The first row(n=0): This row focused on the Adelie Penguins Species and found that 31 instances of the species were correctly predicted and that 0 instances of the species were identified as false negatives as neither a Chinstrap Penguin nor a Gentoo Penguin.
2.   The second row(n=1): This row focused on the Chinstrap Penguins Species and found that 12 instances of the species were correctly predicted and that 1 instances of the species were identified as false negatives in the form of an Adelie Penguin however, there were zero instances of a false positive taking form as a Gentoo Penguin .
3.  The third row(n=2)L This row focused on the Gentoo Penguins Species and found that 23 instances of the species were correctly predicted and that 0 instances of the species were identified as false negatives as neither a Adelie Penguin nor a Chinstrap Penguin.

**Discussion/Interpretation of Random Forests: Cross Validation Score**

The cross validation of the random forest is:  0.9887491264849755. While looking at the cross validation score of the random forest in order to further determine the overall performance of the model, it displayed the average accuracy of 0.9887491264849755 therefore 98.87% of the data was correctly predicted, which can be percieved as an optimal scenario.

**Discussion/Interpretation of Random Forests: Mistakes Made by Model**
According to the recall column of the Chinstrap penguin (Pygoscelis antarctica) species and the Confusion Matrix, 1 Chinstrap Penguin was identified as a Adelie.

## Figure 3: Nearest Neighbor Classifier
"""

# Concatenate train and test data for fitting the LabelEncoder
all_species = pd.concat([train['Species'], test['Species']])

# Fit the LabelEncoder on all unique species
le.fit(all_species)

# Separate features (X) and target (y) in both train and test sets
X = X_train[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex']]
y = y_train

# Ensure X_test has the same feature order as X
X_test = test[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex']]  # Changed line
y_test = test['Species']

# Reset 'Species' to original format if needed
#train['Species'] = le.inverse_transform(train['Species']) # If 'Species' in train was transformed to numeric
#est['Species'] = le.inverse_transform(test['Species'])   # If 'Species' in test was transformed to numeric


# Fit LabelEncoder on combined data with consistent type
all_species = pd.concat([train['Species'], test['Species']])
le.fit(all_species)

# Transform 'Species' in both train and test
y_train_encoded = le.transform(train['Species'])
y_test_encoded = le.transform(test['Species'])

# Update y_train and y_test to encoded values
y_train = y_train_encoded
y_test = y_test_encoded

"""This section of code for Figure 3 further processes the training and test data to encode the 'Species' column into a numeric form. This will allow our machine learning model to cluster and group the different species."""

X.head()

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on training data and transform both training and test data
X_train_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

"""This section of code standardizes the X training data as well as the X_test data. While the Culmen Length and Depth are both in mm, standardizing the data will help remove any outliers and make all the culmen length and depth standardized on the same scale. This is essential, especially for K-nearest neighbor classification and outputting the decision regions without any other outside material from the class."""

# Initialize k-NN classifier
knn = KNeighborsClassifier()

# Define the parameter grid for k
param_grid = {'n_neighbors': np.arange(1, 21)}  # Test k values from 1 to 21

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

# Retrieve the best value of k
best_k = grid_search.best_params_['n_neighbors']
print(f"Best k: {best_k}")

"""After we have encoded our species and standardized all of our data, we create the k-NN model. We first have to import the necessary librariesand then set up the dictionary defined as param_grid. This dictionary tests the values of k from 1 to 20 by setting up an numpy array. We then perform the grid search taking the k-NN model, the search space, and then the number of cross-validation folds to train the model with each value of k in the search space. The output is the k that provides the best accruacy which is 3."""

# Train the k-NN model with the optimal k
knn_best = KNeighborsClassifier(n_neighbors=best_k)
knn_best.fit(X_train_scaled, y_train)

print("Average cross-validation score:", grid_search.best_score_)
# Predict on the test set
y_pred = knn_best.predict(X_test_scaled)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on the test set:", accuracy)

"""After we find the best k to use, we crate and initialize the kNN classifier model. This is done by defining a variable called knn_best which stores the kNN classifier model. The KNeighborsClassifieris a function from sklearn that is used to create the kNN object with the argument being n_neighbors set to the best_k which we found in the previous block of code. The next bit of code runs our kNN model on the training data."""

# Predict on the test set
y_pred = knn_best.predict(X_test_scaled)

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""*   The model correctly predicted 31 instances as Adelie penguins (True Positive = 31).
*   There are 0 instances where the model incorrectly predicted Adelie when it was actually another species (False Positive = 0).
*   There is 1 instance where the model predicted a species other than Adelie (predicted Chinstrap) when it was actually Adelie (False Negative = 1).
*   There are 21 instances where Chinstrap and Gentoo penguins correctly predicted as NOT Adelie (True Negative = 21).


Chinstrap penguins (mapped as 1):

*   The model correctly predicted 12 instances as Chinstrap penguins (True Positive = 12).
*   There are 0 instances where the model incorrectly predicted Chinstrap when it was actually another species (False Positive = 0).
*   There are 0 instances where the model predicted a species other than Chinstrap when it was actually Chinstrap (False Negative = 0).
*   There are 54 instances where Adelie and Gentoo penguins correctly predicted as NOT Chinstrap (True Negative = 54).

Gentoo penguins (mapped as 2):

*   The model correctly predicted 21 instances as Gentoo penguins (True Positive = 21).
*   There is 1 instances where the model incorrectly predicted Gentoo when it was actually  (False Positive = 0).
*   There is 0 instances where the model predicted a species other than Gentoo when it was actually Gentoo (False Negative = 0).
*   There are 45 instances where Adelie and Chinstrap penguins correctly predicted as NOT Gentoo (True Negative = 45).
"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def plot_decision_regions_by_clustering(X, y, model, feature_indices, feature_names, sex_values, title_prefix):
    """
    Plots decision boundaries for a k-NN model for each sex (male and female).

    Parameters:
    X (array-like): Feature matrix with all features.
    y (array-like): Target variable.
    model: Trained k-NN model.
    feature_indices (list): Indices of two features for visualization.
    feature_names (list): Names of the two selected features.
    sex_values (list): Values of 'Sex' to plot separately (e.g., [0, 1]).
    title_prefix (str): Prefix for plot titles.
    """
    for sex in sex_values:
        # Select the two features for visualization
        X_plot = X[:, feature_indices]

        # Create grid for feature space
        x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1
        y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),
                             np.linspace(y_min, y_max, 500))

        # Create grid with all features
        grid = np.zeros((xx.size, X.shape[1]))
        grid[:, feature_indices[0]] = xx.ravel()
        grid[:, feature_indices[1]] = yy.ravel()
        grid[:, 0] = sex  # Set 'Sex' to the current value (0 or 1)

        # Predict grid labels
        Z = model.predict(grid).reshape(xx.shape)

        # Plot decision boundary
        plt.figure(figsize=(10, 6))
        plt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))
        plt.scatter(X_plot[:, 0], X_plot[:, 1], c=y, edgecolor='k', cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))
        plt.title(f"{title_prefix} for Sex = {'Male' if sex == 0 else 'Female'}")
        plt.xlabel(feature_names[0])
        plt.ylabel(feature_names[1])
        plt.show()

"""The docstrings and comments explain the arguments and general overall code of the function to plot the decision regions."""

# Plot decision boundaries for 'Sex = 0' (male) and 'Sex = 1' (female)
plot_decision_regions_by_clustering(
    X=X_test_scaled,
    y=y_test,
    model=knn_best,
    feature_indices=[1, 2],  # Indices for 'Culmen Length' and 'Culmen Depth'
    feature_names=["Culmen Length (Standardized)", "Culmen Depth (Standardized)"],
    sex_values=[0, 1],  # Male and Female
    title_prefix="Decision Boundary for k-NN Classification"
)

"""Looking at the decision regions, we can see some unusual plotting. The reason that these graphs look like this is because of the standardization that had previously occurred. However, we can parse out that Adelie penguins on average have smaller culmen lengths after standardization in both male and female sexes. As for classification for the Chinstrap and Gentoo penguins, the model doesn't work as well. As we can see, the region for Chinstrap as colored by red, covers half of the decision region in the second visualization. While the Gentoo region is shown in the first graph, it is only clustered around the lower right hand side of the graph. This suggests that the graph, while accurate might not be the best model determining what type of species each penguin is based solely on culmen length, depth, and sex. A better model may predict each species better or a different selection of features may help contribute to a clearer decision region. However, this model does predict what species each penguin is with relatively high accuracy and could be used if there were no other models.

# Conclusion: Overall Discussion & Recommendation

# **Discussion on Performance of the Three Models**


1.   **Figure 1: Multinomial Logistic regression Model**
*   Cross Validation Score:
*   Accuracy Score: 0.99245
* Classification Report:
  * Weighted Average Percision:0.99
  * Weighted Average Recall:0.98
  * Weighted Average F1-Score:0.98
  * Weighted Average Support:67




2.   **Figure 2: Random Forests Model**
*   Cross Validation Score: 0.9887491264849755
*   Accuracy Score: 0.9850746268656716
* Classification Report:
  * Weighted Average Percision:1.00
  * Weighted Average Recall:0.98
  * Weighted Average F1-Score:0.99
  * Weighted Average Support:67

3. **Figure : Nearest Neighbor Classifier Model**
*   Cross Validation Score: 0.9925925925925926
*   Accuracy Score: 0.9850746268656716
* Classification Report:
  * Weighted Average Percision:0.99
  * Weighted Average Recall:0.98
  * Weighted Average F1-Score:0.99
  * Weighted Average Support:67

**Recommended Model**
  Thus following further observation of the figures, the Random Forest Model has the most optial performance, in comparisson to the Nearest Neighbor Classifier Model and Multinomial Logistic regression Model.

  Recommended Features:

Likewise following anaysis of the data,the features that would be optimal are:

1. Delta 15 N (o/oo)
2. Island
3. Culmen Length (mm)
4. Culmen Depth (mm)

**Future Improvements**

As the penguin population continues to grow, the data set should further develop thus allow for the machiene learning figures to have improvements in regards to accuracy.
"""